# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Optional: Change the model name to match your installed Ollama model
# Available models: llama2, mistral, neural-chat, dolphin-mixtral, llama3.2, etc.
LLM_MODEL=llama3.2
